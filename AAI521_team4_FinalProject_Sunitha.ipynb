{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FOOD DETECTION\n",
        "================================================================\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# Mount Google Drive\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Smart Dataset Setup (Downloads only if not in Drive)\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Google Drive paths\n",
        "DRIVE_DATA_DIR = Path(\"/content/drive/MyDrive/AAI521-Final-Project/data/food-101\")\n",
        "IMAGES_DIR = DRIVE_DATA_DIR / \"images\"\n",
        "META_DIR = DRIVE_DATA_DIR / \"meta\"\n",
        "\n",
        "# Check if dataset exists\n",
        "if IMAGES_DIR.exists() and META_DIR.exists():\n",
        "    print(\"Dataset found in Google Drive - using existing copy!\")\n",
        "    print(f\"   Classes: {len([d for d in IMAGES_DIR.iterdir() if d.is_dir()])}\")\n",
        "else:\n",
        "    print(\"First time - downloading dataset (10-15 min, only once)...\")\n",
        "\n",
        "    import kagglehub\n",
        "    cache_path = kagglehub.dataset_download(\"dansbecker/food-101\")\n",
        "\n",
        "    # Find dataset in cache\n",
        "    cache_base = Path(cache_path)\n",
        "    images_dirs = list(cache_base.rglob(\"images\"))\n",
        "\n",
        "    if images_dirs:\n",
        "        source_images = images_dirs[0]\n",
        "        source_meta = source_images.parent / \"meta\"\n",
        "\n",
        "        # Copy to Google Drive\n",
        "        DRIVE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        print(\"Copying to Google Drive...\")\n",
        "        shutil.copytree(source_images, IMAGES_DIR)\n",
        "        shutil.copytree(source_meta, META_DIR)\n",
        "        print(\"Dataset saved to Google Drive!\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Could not find dataset\")\n",
        "\n",
        "print(f\"Ready! Images: {IMAGES_DIR}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Install & Import Libraries\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q tensorflow opencv-python scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import json\n",
        "import warnings\n",
        "from collections import Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"GPU:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "print(\"Libraries loaded!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/AAI521-Final-Project/outputs\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_CNN = 20\n",
        "EPOCHS_FINETUNE = 10\n",
        "LEARNING_RATE = 0.001\n",
        "SEED = 42\n",
        "\n",
        "print(f\"Outputs will be saved to: {SAVE_DIR}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Calorie Database\n",
        "# =============================================================================\n",
        "\n",
        "CALORIE_DB = {\n",
        "    'apple_pie': 237, 'baby_back_ribs': 361, 'baklava': 428, 'beef_carpaccio': 120,\n",
        "    'beef_tartare': 180, 'beet_salad': 89, 'beignets': 303, 'bibimbap': 490,\n",
        "    'bread_pudding': 310, 'breakfast_burrito': 350, 'bruschetta': 109, 'caesar_salad': 184,\n",
        "    'cannoli': 300, 'caprese_salad': 108, 'carrot_cake': 435, 'ceviche': 120,\n",
        "    'cheesecake': 321, 'cheese_plate': 150, 'chicken_curry': 250, 'chicken_quesadilla': 529,\n",
        "    'chicken_wings': 290, 'chocolate_cake': 352, 'chocolate_mousse': 225, 'churros': 320,\n",
        "    'clam_chowder': 236, 'club_sandwich': 590, 'crab_cakes': 160, 'creme_brulee': 294,\n",
        "    'croque_madame': 456, 'cup_cakes': 305, 'deviled_eggs': 62, 'donuts': 269,\n",
        "    'dumplings': 41, 'edamame': 122, 'eggs_benedict': 450, 'escargots': 142,\n",
        "    'falafel': 333, 'filet_mignon': 227, 'fish_and_chips': 585, 'foie_gras': 462,\n",
        "    'french_fries': 312, 'french_onion_soup': 330, 'french_toast': 220, 'fried_calamari': 175,\n",
        "    'fried_rice': 228, 'frozen_yogurt': 127, 'garlic_bread': 150, 'gnocchi': 250,\n",
        "    'greek_salad': 106, 'grilled_cheese_sandwich': 440, 'grilled_salmon': 367, 'guacamole': 150,\n",
        "    'gyoza': 64, 'hamburger': 354, 'hot_and_sour_soup': 91, 'hot_dog': 290,\n",
        "    'huevos_rancheros': 400, 'hummus': 166, 'ice_cream': 207, 'lasagna': 378,\n",
        "    'lobster_bisque': 200, 'lobster_roll_sandwich': 436, 'macaroni_and_cheese': 320,\n",
        "    'macarons': 95, 'miso_soup': 40, 'mussels': 86, 'nachos': 346,\n",
        "    'omelette': 154, 'onion_rings': 276, 'oysters': 68, 'pad_thai': 380,\n",
        "    'paella': 372, 'pancakes': 227, 'panna_cotta': 250, 'peking_duck': 337,\n",
        "    'pho': 350, 'pizza': 266, 'pork_chop': 231, 'poutine': 740,\n",
        "    'prime_rib': 361, 'pulled_pork_sandwich': 450, 'ramen': 436, 'ravioli': 220,\n",
        "    'red_velvet_cake': 478, 'risotto': 320, 'samosa': 262, 'sashimi': 127,\n",
        "    'scallops': 137, 'seaweed_salad': 45, 'shrimp_and_grits': 363, 'spaghetti_bolognese': 350,\n",
        "    'spaghetti_carbonara': 400, 'spring_rolls': 140, 'steak': 271, 'strawberry_shortcake': 350,\n",
        "    'sushi': 143, 'tacos': 226, 'takoyaki': 103, 'tiramisu': 240,\n",
        "    'tuna_tartare': 145, 'waffles': 291\n",
        "}\n",
        "\n",
        "print(f\"Calorie database: {len(CALORIE_DB)} foods\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Load Dataset\n",
        "# =============================================================================\n",
        "\n",
        "def read_list(p):\n",
        "    with open(p, 'r') as f:\n",
        "        return [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "train_list = read_list(META_DIR / \"train.txt\")\n",
        "test_list = read_list(META_DIR / \"test.txt\")\n",
        "\n",
        "class_names = sorted([p.name for p in IMAGES_DIR.iterdir() if p.is_dir()])\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_classes = [s.split(\"/\")[0] for s in train_list]\n",
        "train_counts = Counter(train_classes)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training samples: {len(train_list)}\")\n",
        "print(f\"Test samples: {len(test_list)}\")\n",
        "print(f\"Classes: {num_classes}\")\n",
        "print(f\"Per class: {len(train_list) // num_classes} train, {len(test_list) // num_classes} test\")\n",
        "\n",
        "# Visualize distribution\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.bar(range(len(train_counts)), train_counts.values())\n",
        "plt.axhline(np.mean(list(train_counts.values())), color='r', linestyle='--')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Samples')\n",
        "plt.title('Training Data Distribution')\n",
        "plt.savefig(SAVE_DIR / 'class_distribution.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Create Data Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "def create_path_label_pairs(file_list):\n",
        "    pairs = []\n",
        "    for stem in file_list:\n",
        "        cls, base = stem.split(\"/\", 1)\n",
        "        path = str(IMAGES_DIR / cls / f\"{base}.jpg\")\n",
        "        label = class_to_idx[cls]\n",
        "        pairs.append((path, label))\n",
        "    return pairs\n",
        "\n",
        "train_pairs = create_path_label_pairs(train_list)\n",
        "test_pairs = create_path_label_pairs(test_list)\n",
        "\n",
        "def decode_resize(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "augment = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.15),\n",
        "    layers.RandomZoom(0.15),\n",
        "    layers.RandomContrast(0.15),\n",
        "    layers.RandomBrightness(0.15),\n",
        "], name=\"augmentation\")\n",
        "\n",
        "train_paths, train_labels = zip(*train_pairs)\n",
        "test_paths, test_labels = zip(*test_pairs)\n",
        "\n",
        "train_ds = (tf.data.Dataset\n",
        "           .from_tensor_slices((list(train_paths), list(train_labels)))\n",
        "           .shuffle(len(train_pairs), seed=SEED)\n",
        "           .map(decode_resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "           .batch(BATCH_SIZE)\n",
        "           .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "train_ds = train_ds.map(lambda img, label: (augment(img, training=True), label))\n",
        "\n",
        "test_ds = (tf.data.Dataset\n",
        "          .from_tensor_slices((list(test_paths), list(test_labels)))\n",
        "          .map(decode_resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "          .batch(BATCH_SIZE)\n",
        "          .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(f\"Train batches: {len(train_ds)}, Test batches: {len(test_ds)}\")\n",
        "\n",
        "# Show samples\n",
        "for images, labels in train_ds.take(1):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        img = (images[i].numpy() + 1) / 2.0\n",
        "        plt.imshow(img)\n",
        "        plt.title(class_names[labels[i].numpy()].replace('_', ' '), fontsize=9)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle('Sample Training Images')\n",
        "    plt.savefig(SAVE_DIR / 'sample_images.png', dpi=100, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Build CNN Model (Custom Top-5 Metric)\n",
        "# =============================================================================\n",
        "\n",
        "# Custom Top-5 Accuracy Metric that works with sparse labels\n",
        "class SparseTop5CategoricalAccuracy(keras.metrics.Metric):\n",
        "    def __init__(self, name='top5_acc', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # y_true shape: (batch_size,) - sparse labels\n",
        "        # y_pred shape: (batch_size, num_classes) - predictions\n",
        "\n",
        "        # Get top 5 predictions\n",
        "        top5_pred = tf.math.top_k(y_pred, k=5).indices\n",
        "\n",
        "        # Expand y_true to match shape for comparison\n",
        "        y_true_expanded = tf.expand_dims(tf.cast(y_true, tf.int32), axis=-1)\n",
        "\n",
        "        # Check if true label is in top 5\n",
        "        matches = tf.reduce_any(tf.equal(top5_pred, y_true_expanded), axis=-1)\n",
        "        matches = tf.cast(matches, tf.float32)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            matches = matches * sample_weight\n",
        "            self.count.assign_add(tf.reduce_sum(sample_weight))\n",
        "        else:\n",
        "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
        "\n",
        "        self.total.assign_add(tf.reduce_sum(matches))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total.assign(0.)\n",
        "        self.count.assign(0.)\n",
        "\n",
        "\n",
        "def build_cnn():\n",
        "    inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
        "    base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(*IMG_SIZE, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy', SparseTop5CategoricalAccuracy()]  # FIXED: Using custom metric\n",
        "    )\n",
        "    return model, base_model\n",
        "\n",
        "cnn_model, base_model = build_cnn()\n",
        "print(\"CNN model built with fixed Top-5 metric\")\n",
        "cnn_model.summary()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Train CNN\n",
        "# =============================================================================\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
        "    ModelCheckpoint(str(SAVE_DIR / 'best_cnn_stage1.keras'), monitor='val_accuracy', save_best_only=True)\n",
        "]\n",
        "\n",
        "print(\"Training CNN Stage 1 (frozen base)...\")\n",
        "history = cnn_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS_CNN,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].plot(history.history['accuracy'], label='Train')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Val')\n",
        "axes[0].set_title('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history['loss'], label='Train')\n",
        "axes[1].plot(history.history['val_loss'], label='Val')\n",
        "axes[1].set_title('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Stage 1 Training')\n",
        "plt.savefig(SAVE_DIR / 'training_stage1.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Stage 1 complete!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Fine-tune CNN\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Fine-tuning CNN (unfreezing top layers)...\")\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(LEARNING_RATE / 10),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy', SparseTop5CategoricalAccuracy()]  # FIXED: Using custom metric\n",
        ")\n",
        "\n",
        "callbacks_ft = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint(str(SAVE_DIR / 'best_cnn_finetuned.keras'), monitor='val_accuracy', save_best_only=True)\n",
        "]\n",
        "\n",
        "history_ft = cnn_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS_FINETUNE,\n",
        "    callbacks=callbacks_ft,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "axes[0].plot(history_ft.history['accuracy'], label='Train')\n",
        "axes[0].plot(history_ft.history['val_accuracy'], label='Val')\n",
        "axes[0].set_title('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].plot(history_ft.history['loss'], label='Train')\n",
        "axes[1].plot(history_ft.history['val_loss'], label='Val')\n",
        "axes[1].set_title('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Stage 2 Fine-tuning')\n",
        "plt.savefig(SAVE_DIR / 'training_stage2.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Evaluate CNN\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Evaluating model...\")\n",
        "results = cnn_model.evaluate(test_ds, verbose=0)\n",
        "metrics = dict(zip(cnn_model.metrics_names, results))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL TEST RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Visualize predictions\n",
        "images, labels = next(iter(test_ds.take(1)))\n",
        "predictions = cnn_model.predict(images[:16], verbose=0)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = (images[i].numpy() + 1) / 2.0\n",
        "    true_label = class_names[labels[i].numpy()]\n",
        "    pred_label = class_names[np.argmax(predictions[i])]\n",
        "    conf = np.max(predictions[i])\n",
        "\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f'True: {true_label}\\nPred: {pred_label}\\n{conf:.2f}', color=color, fontsize=8)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Model Predictions')\n",
        "plt.savefig(SAVE_DIR / 'predictions.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Prediction Function\n",
        "# =============================================================================\n",
        "\n",
        "def predict_food(image_path, model, top_k=5):\n",
        "    \"\"\"Predict food and calories\"\"\"\n",
        "    img = tf.io.read_file(str(image_path))\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = preprocess_input(img)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "\n",
        "    predictions = model.predict(img, verbose=0)\n",
        "    top_indices = np.argsort(predictions[0])[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        food_name = class_names[idx]\n",
        "        confidence = predictions[0][idx]\n",
        "        calories = CALORIE_DB.get(food_name, \"Unknown\")\n",
        "        results.append({\n",
        "            'food': food_name.replace('_', ' ').title(),\n",
        "            'confidence': f\"{confidence * 100:.2f}%\",\n",
        "            'calories': calories\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(\"Prediction function ready!\")\n",
        "print(\"\\nUsage: results = predict_food('image.jpg', cnn_model)\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Test Prediction\n",
        "# =============================================================================\n",
        "\n",
        "# Get a test image\n",
        "test_images, test_labels = next(iter(test_ds.take(1)))\n",
        "sample_idx = 0\n",
        "\n",
        "# Save temp image\n",
        "sample_img = test_images[sample_idx]\n",
        "sample_img_uint8 = tf.cast((sample_img + 1) * 127.5, tf.uint8)\n",
        "temp_path = SAVE_DIR / 'temp_test.jpg'\n",
        "tf.io.write_file(str(temp_path), tf.image.encode_jpeg(sample_img_uint8))\n",
        "\n",
        "# Predict\n",
        "results = predict_food(temp_path, cnn_model)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREDICTION DEMO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Actual: {class_names[test_labels[sample_idx].numpy()]}\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"{i}. {r['food']}\")\n",
        "    print(f\"   Confidence: {r['confidence']}\")\n",
        "    print(f\"   Calories: {r['calories']} kcal/100g\\n\")\n",
        "\n",
        "# Visualize\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "ax1.imshow((test_images[sample_idx].numpy() + 1) / 2.0)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Test Image')\n",
        "\n",
        "ax2.axis('off')\n",
        "top = results[0]\n",
        "text = f\"{top['food']}\\n\\n\"\n",
        "text += f\"Confidence: {top['confidence']}\\n\"\n",
        "text += f\"Calories: {top['calories']} kcal/100g\"\n",
        "ax2.text(0.5, 0.7, text, ha='center', va='center', fontsize=14,\n",
        "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
        "\n",
        "others = \"\\n\".join([f\"{i}. {r['food']} ({r['confidence']})\"\n",
        "                     for i, r in enumerate(results[1:], 2)])\n",
        "ax2.text(0.5, 0.3, others, ha='center', va='top', fontsize=10,\n",
        "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
        "\n",
        "plt.savefig(SAVE_DIR / 'prediction_demo.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Save Results\n",
        "# =============================================================================\n",
        "\n",
        "summary = {\n",
        "    'dataset': {\n",
        "        'classes': num_classes,\n",
        "        'train_samples': len(train_list),\n",
        "        'test_samples': len(test_list)\n",
        "    },\n",
        "    'performance': {\n",
        "        'test_loss': float(metrics['loss']),\n",
        "        'test_accuracy': float(metrics['accuracy']),\n",
        "        'test_top5_accuracy': float(metrics['top5_acc'])\n",
        "    },\n",
        "    'config': {\n",
        "        'img_size': IMG_SIZE,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'epochs_cnn': EPOCHS_CNN,\n",
        "        'epochs_finetune': EPOCHS_FINETUNE\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(SAVE_DIR / 'results.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PROJECT COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAll files saved to: {SAVE_DIR}\")\n",
        "print(\"\\nGenerated files:\")\n",
        "print(\"  best_cnn_stage1.keras\")\n",
        "print(\"  best_cnn_finetuned.keras (‚Üê Use this for predictions)\")\n",
        "print(\"  class_distribution.png\")\n",
        "print(\"  sample_images.png\")\n",
        "print(\"  training_stage1.png\")\n",
        "print(\"  training_stage2.png\")\n",
        "print(\"  predictions.png\")\n",
        "print(\"  prediction_demo.png\")\n",
        "print(\"  results.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HOzcoMxamWWL",
        "outputId": "c10d801c-3250-4027-e6a9-c221c62cf0a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2429889560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Drive mounted!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ]
}